{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Continuous Integration & Code Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Integration\n",
    "\n",
    "<center>![](continuous_integration.jpg)</center>\n",
    "\n",
    "### Why?\n",
    "* So you've written a set of tests for your code, you run them and everything passes - great! However, you then go back to work on your code and quickly forget about testing it. Eventually, a few months later after implementing several new features you remember to try testing your code again. You run the set of tests, only to find that they fail \n",
    "* Solution: continuous integration.\n",
    "* This will run your tests for you regularly (e.g. every night, every time you push changes to a repository) and report back to you the results\n",
    "* Can now spot (almost) instantly when code breaks \n",
    "\n",
    "**I am going to use travis as circle CI does not have documentation for C++.**\n",
    "\n",
    "So, you've written a set of tests for your code, you run them and everything passes - great! However, you then go back to work on developing your code and quickly forget about testing it. Eventually, a few months later, after implementing several new features and refactoring several important functions, you remember to try testing your code again. You run the set of tests, only to find that they fail. \n",
    "\n",
    "This could have been prevented if you had run your tests regularly, updating them often so that they reflect changes to your code. However, remembering to do this manually is less than ideal, plus it would be nice to have some kind of log of test results for each version of your code so you can avoid repeating past mistakes (as you can see what things broke the code in the past and avoid doing that again). \n",
    "\n",
    "The solution to this is continuous integration. This is a way of automating your tests so that they are run regularly (e.g. every night, every time you push changes to a repository), generating reports of test results for you to peruse at leisure and notifying you (almost) instantly when tests fail. Assuming you have a comprehensive, effective set of tests, you can now go ahead and develop your code safe in the knowledge that if you break something, you should find out almost as soon as you have committed (and pushed) the change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travis CI\n",
    "\n",
    "[Travis CI](https://travis-ci.com) is a remote continuous integration service that can easily be linked to your GitHub repository so that every time you push a change your tests get executed. It will then generate a report detailing which tests passed and which failed and (if you wish) send you an email letting you know what happened. \n",
    "\n",
    "For simple python projects, Travis CI is pretty simple to set up. First, set up your account by [signing in](https://travis-ci.org/auth) to Travis CI with your GitHub account. Then go to your [profile page](https://travis-ci.org/profile) and enable Travis CI for your chosen repository.\n",
    "\n",
    "Next, you need to tell Travis what to do to test your project. This is done by creating a `.travis.yml` file in your repository. For a python project, this file will look something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .travis.yml\n",
    "```yml\n",
    "language: python\n",
    "python: \n",
    "    - \"2.7\"\n",
    "    - \"3.5\"\n",
    "# command to install dependencies\n",
    "install: \n",
    "    - pip install -r requirements.txt\n",
    "# command to run tests\n",
    "script: pytest # or py.test for Python versions 3.5 and below\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will install the python packages listed in the file `requirements.txt`, then execute `pytest`. It will test the code for python versions 2.7 and 3.5. \n",
    "\n",
    "Finally, add this `.travis.yml` file to git, commit and push it to the remote repository. This will trigger a Travis CI build. You can check to see if the build passes or fails by checking the [build status](https://travis-ci.org/repositories) page.\n",
    "\n",
    "Unfortunately, packages such as `numpy` are not included in the default python environment where travis does its testing. Therefore, if your project includes packages from the anaconda distribution, you'll need a [slightly more complicated script](http://conda-test.pydata.org/docs/travis.html) such as the one below. This downloads and installs a barebones version of anaconda, `miniconda`, onto the virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .travis.yml\n",
    "```yml\n",
    "language: python\n",
    "python:\n",
    "  - \"2.7\"\n",
    "  - \"3.5\"\n",
    "before_install:\n",
    "  - sudo apt-get update\n",
    "# Setup anaconda\n",
    "install:\n",
    "  # We do this conditionally because it saves us some downloading if the\n",
    "  # version is the same.\n",
    "  - if [[ \"$TRAVIS_PYTHON_VERSION\" == \"2.7\" ]]; then\n",
    "      wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh -O miniconda.sh;\n",
    "    else\n",
    "      wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;\n",
    "    fi\n",
    "  - bash miniconda.sh -b -p $HOME/miniconda\n",
    "  - export PATH=\"$HOME/miniconda/bin:$PATH\"\n",
    "  - hash -r\n",
    "  - conda config --set always_yes yes --set changeps1 no\n",
    "  - conda update -q conda\n",
    "  # Useful for debugging any issues with conda\n",
    "  - conda info -a\n",
    "  - conda create -q -n test-environment python=$TRAVIS_PYTHON_VERSION numpy scipy matplotlib ipython nose\n",
    "  - source activate test-environment\n",
    "  - pip install -r requirements.txt\n",
    "# Run test\n",
    "script:\n",
    "  - pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced features for customising your travis build and a description of the travis build lifecycle can be found [here](https://docs.travis-ci.com/user/customizing-the-build/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local CI\n",
    "\n",
    "Remote continuous integration services are great for open source codes, however if your code is closed or uses non-standard hardware (e.g. GPUs), you may want instead to run your tests locally. Rather than manually running these tests every time you make changes to your code, you can use tools such as [`jenkins`](https://jenkins.io) to manage this for you. `Jenkins` is an automation server which can be used to automate a variety of tasks including building and testing. Once set up, much like `travis.ci`, it will automatically run your tests every time you commit changes to your code and generate reports on whether your tests passed or failed. \n",
    "\n",
    "\n",
    "### How?\n",
    "* Several tools out there: `travis.ci`, `jenkins`, `circle.ci`\n",
    "* Involve writing a short script which details the computational setup (i.e. any libraries needed) and what code should be run to execute tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code converage\n",
    "### Why?\n",
    "* So you have a test suite and you're using continuous integration to run it regularly\n",
    "* However, how do you know that you are testing all parts of your code? It's all very well to test a few auxilliary functions, but if you're not testing the main part of the code then you still cannot trust the results\n",
    "* Solution: code coverage\n",
    "* This will track what parts of the code are being run when tests execute and will highlight areas not currently being tested\n",
    "* Generally want to aim for > 90% code coverage\n",
    "\n",
    "### How?\n",
    "* There exist libraries for most languages that will produce code coverage reports, e.g. `coverage.py` for python, `gcov` for C/C++, `tcov` for C/C++/fortran\n",
    "* Can use tools like Codecov to integrate these tools with continuous integration, providing an easy-to-use interface to analyse code coverage and keep track of code coverage as develop code\n",
    "* These tools are also particularly useful if code is written in multiple languages, as will combine reports produced for each of the different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you have written a test suite and are using continuous integration to run it regularly - awesome! However, how do you know that you are testing all parts of your code? If your tests only cover 20% of the code that executes, that is no guarantee that the other 80% is doing what it's supposed to, and therefore you still cannot trust your code's results. Fortunately, rather than trawling through your code, inspecting each line to see if it's being tested, there are *code coverage* tools that can automate this. These tools can be linked with your continuous integration service so that they are run when your tests execute, generating a `.coveragerc` report and allowing you to spot parts of your code that could do with some more testing. \n",
    "\n",
    "There are code coverage libraries for most languages that will generate these coverage reports: `coverage.py` for python, `gcov` for C/C++, `tcov` for C/C++/fortran. [Codecov](https://codecov.io/) is a nice tool that can then be used to analyse these coverage reports. It keeps track of code coverage for each version of the code, providing a graphical interface which highlights which exact lines are covered / not covered by testing, and generally provides a lot of graphs and features which make monitoring the coverage of your tests a lot easier.\n",
    "\n",
    "To run codecov on a python project, modify your `.travis.yml` file as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### With pytest\n",
    "\n",
    "```yml\n",
    "language: python\n",
    "python: \n",
    "    - \"2.7\"\n",
    "    - \"3.5\"\n",
    "# command to install dependencies\n",
    "install: \n",
    "    - pip install codecov\n",
    "    - pip install pytest-cov\n",
    "# command to run tests\n",
    "script: \n",
    "    - pytest --cov=./ # or py.test for Python versions 3.5 and below\n",
    "after_sucess:\n",
    "    - codecov\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With nose\n",
    "\n",
    "```yml\n",
    "language: python\n",
    "python: \n",
    "    - \"2.7\"\n",
    "    - \"3.5\"\n",
    "# command to install dependencies\n",
    "install: \n",
    "    - pip install codecov\n",
    "    - pip install coverage\n",
    "# command to run tests\n",
    "script: \n",
    "    - nosetests -- with-coverage\n",
    "after_sucess:\n",
    "    - codecov\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read more\n",
    "- [Guide](https://docs.travis-ci.com/user/languages/python/) for building python projects in travis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
